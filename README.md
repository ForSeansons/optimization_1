# LASSO å›å½’ä¼˜åŒ–ç®—æ³•ï¼šåŠ¨æ€ç­–ç•¥å…¨æ™¯

## ğŸ“˜ é¡¹ç›®ç®€ä»‹ (Introduction)

å›´ç»• **LASSO (Least Absolute Shrinkage and Selection Operator)** çš„æ±‚è§£ï¼Œæœ¬é¡¹ç›®åœ¨åŒä¸€ä»£ç æ¡†æ¶ä¸‹ç³»ç»Ÿæ¯”è¾ƒäº†å¤šç§ç»å…¸ä¸æ”¹è¿›å‹ç®—æ³•çš„æ”¶æ•›è¡Œä¸ºã€‚åˆ†æèšç„¦äºï¼š

* **å¯åˆ†ç»“æ„**ï¼ˆåæ ‡ä¸‹é™ã€åŸå§‹-å¯¹å¶æ··åˆç­‰ï¼‰ä¸ **è¿‘ç«¯/æ¬¡æ¢¯åº¦ç­–ç•¥**ï¼ˆISTAã€FISTAã€Huberã€PDHGã€éšæœºè¿‘ç«¯æ›´æ–°ï¼‰ã€‚
* **åŠ¨æ€æŠ€å·§**ï¼ˆè·¯å¾„è¿½è¸ªã€Active-setã€åŠ¨é‡é‡å¯ã€å‚æ•°ç»­æ¥ã€åŸå§‹â€”å¯¹å¶è€¦åˆï¼‰å¯¹æ”¶æ•›é€Ÿåº¦ä¸ç¨³å®šæ€§çš„å½±å“ã€‚
* **ä½ç»´å¸¸è§„åœºæ™¯**ä¸ **é«˜ç»´å¼ºç—…æ€åœºæ™¯** çš„å·®å¼‚ï¼Œä»¥åŠä¸åŒç®—æ³•åœ¨ suboptimality è§†è§’ä¸‹çš„å®Œæ•´è¿­ä»£è½¨è¿¹ã€‚

ç›®æ ‡å‡½æ•°é‡‡å–æ ‡å‡†å½¢å¼ï¼š

$$
\min_{\beta} \frac{1}{2n}\left\|y - X\beta\right\|_2^2 + \lambda \left\|\beta\right\|_1
$$

## ğŸ§ª å®éªŒè®¾ç½® (Experimental Setup)

| è®¾ç½®é¡¹ | åœºæ™¯ Aï¼šä½ç»´ä½ç—…æ€ (Low-Dim) | åœºæ™¯ Bï¼šé«˜ç»´é«˜ç—…æ€ (High-Dim) |
| :--- | :--- | :--- |
| **æ ·æœ¬æ•° (n)** | 200 | 200 |
| **ç‰¹å¾æ•° (p)** | 50 | **1000** |
| **çœŸå€¼ç¨€ç–åº¦** | å‰ 10 ä¸ªç³»æ•°éé›¶ | å‰ 10 ä¸ªç³»æ•°éé›¶ |
| **æ­£åˆ™åŒ–æƒé‡** | $\lambda = 0.1\,\lambda_{\max}$ | $\lambda = 0.1\,\lambda_{\max}$ |
| **æœ€å¤§è¿­ä»£** | 100 | 250 |
| **é‡å¤å®éªŒ** | 100 æ¬¡ç‹¬ç«‹è¯•éªŒ | 10 æ¬¡ç‹¬ç«‹è¯•éªŒ |
| **æ”¶æ•›åˆ¤æ®** | $f(x_k) - f^\star \le 10^{-6}$ | åŒå·¦ |

è¡¥å……è¯´æ˜ï¼š

* $f^\star$ ç”±é«˜ç²¾åº¦ scikit-learn Lasso æ±‚è§£å¾—åˆ°ã€‚
* ç»†çº¿è¡¨ç¤ºå•æ¬¡è¯•éªŒçš„è½¨è¿¹ï¼ˆcloud plotï¼‰ï¼Œç²—çº¿ä¸ºå‡å€¼ã€‚
* æ‰€æœ‰å®éªŒæ•°æ®ã€å¯è§†åŒ–åŠç»Ÿè®¡å‡ä½äº `verson_2/` ç›®å½•ï¼Œä¾¿äºå¤ç°åŠå¼•ç”¨ã€‚

## ğŸš€ å®ç°ç®—æ³• (Implemented Algorithms)

1. **Coordinate Descent ç³»åˆ—**ï¼šæ ‡å‡†åæ ‡ä¸‹é™ä¸ Pathwise+Active-set ç­–ç•¥ï¼ˆæ²¿ $\lambda$ è·¯å¾„ warm-start å¹¶å‘¨æœŸæ€§ KKT æ‰«æï¼‰ã€‚
2. **Huber Gradient å®¶æ—**ï¼šåŸå§‹ã€åŠ é€Ÿã€åŠ é€Ÿ+é‡å¯ä¸‰ç§å½¢æ€ï¼Œç”¨å¹³æ»‘è¿‘ä¼¼è¿½è¸ª L1 ç›®æ ‡ã€‚
3. **FISTA / ISTA**ï¼šå«åŠ¨é‡é‡å¯ (FISTA-R) çš„ Nesterov è¿‘ç«¯æ¢¯åº¦ï¼Œä»¥åŠåŸºçº¿ ISTAã€‚
4. **ADMM ($\rho \in \{0.5,1,2,5\}$)**ï¼šè€ƒå¯Ÿæƒ©ç½šå‚æ•°å¯¹é€Ÿåº¦ä¸ç¨³å®šæ€§çš„å½±å“ã€‚
5. **Subgradient å®¶æ—**ï¼šç»å…¸æ¬¡æ¢¯åº¦ã€continuation æ¬¡æ¢¯åº¦ã€éšæœºæ¬¡æ¢¯åº¦ã€‚
6. **Stochastic Proximal Gradient (SPG)**ï¼šmini-batch è¿‘ç«¯æ¢¯åº¦ï¼Œä¸éšæœºæ¬¡æ¢¯åº¦å½¢æˆå¯¹ç…§ã€‚
7. **Primal-Dual Hybrid Gradient (PDHG)**ï¼šChambolle-Pock å¼åŸå§‹å¯¹å¶è€¦åˆè¿­ä»£ã€‚

## ğŸ§  ç®—æ³•è§£è¯» (Algorithm Notes)

- **Coordinate Descent (CD)**ï¼šä¾æ¬¡æœ€å°åŒ–å•ä¸ªåæ ‡å­é—®é¢˜ï¼Œå¹¶ç”¨ $\beta_j \leftarrow S_{\lambda/A_j}(c_j/A_j)$ çš„è½¯é˜ˆå€¼æ›´æ–°å±•ç¤º L1 å¯åˆ†ç»“æ„ (Sequential coordinate-wise minimization)ã€‚
- **Coordinate Desc (Pathwise+Active)**ï¼šæ²¿å‡ ä½•é€’å‡çš„ $\lambda$ è·¯å¾„ warm-startï¼Œå¹¶åªåœ¨æ¿€æ´»é›†ä¸å‘¨æœŸæ€§ KKT æ‰«æä¸Šå¾ªç¯ï¼Œç­‰ä»·äº homotopy + screening çš„ç»„åˆ (Homotopy warm starts plus active screening)ã€‚
- **Huber Gradient / Accel / Restart**ï¼šç”¨ Huber å¹³æ»‘é¡¹ $h_{\delta}(\beta)$ æ›¿ä»£ L1ï¼Œæ¯”è¾ƒæ ‡å‡†ã€åŠ é€Ÿã€åŠ é€Ÿ+é‡å¯æ¢¯åº¦åœ¨ $\nabla f + \lambda \nabla h_{\delta}$ ä¸Šçš„è¡Œä¸º (Smooth approximation under classic/accelerated/restart schemes)ã€‚
- **FISTA**ï¼šNesterov åŠ¨é‡ä¸è½¯é˜ˆå€¼è¿‘ç«¯ç»“åˆï¼Œä¾é  $t_{k+1}$ æ§åˆ¶åŠ é€Ÿé¡¹ä»¥å®ç° $O(1/k^2)$ æ”¶æ•› (Momentum-accelerated proximal gradient)ã€‚
- **FISTA (Restart)**ï¼šå½“ $\langle z_k-\beta_{k+1}, \beta_{k+1}-\beta_k\rangle > 0$ æ—¶é‡ç½®åŠ¨é‡ï¼Œæ¶ˆé™¤â€œé”¯é½¿â€å¹¶ä¿æŒ FISTA çš„å¿«é€Ÿè¡°å‡ (Adaptive restart to suppress oscillations)ã€‚
- **Proximal Gradient (ISTA)**ï¼šå›ºå®šæ­¥é•¿ $1/L$ é…åˆè½¯é˜ˆå€¼ä½œä¸ºæ— åŠ¨é‡çš„è¿‘ç«¯åŸºçº¿ï¼Œæä¾›å¹³æ»‘ä½†ç¨æ…¢çš„ä¸‹é™ (Baseline proximal updates without momentum)ã€‚
- **ADMM ($\rho=0.5/1/2/5$)**ï¼šé€šè¿‡ä»¥ä¸‹å…¬å¼å®ç°åŸå§‹-è¿‘ç«¯åˆ†å—æ›´æ–°ï¼Œä¸åŒ $\rho$ å†³å®šæ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ (Classical splitting with varying penalty strength)ï¼š
  $$
  \begin{aligned}
  \beta^{k+1} &= \left(X^\top X / n + \rho I\right)^{-1} \left(X^\top y / n + \rho(z^k - u^k)\right) \\
  z^{k+1} &= S_{\lambda/\rho}\left(\beta^{k+1} + u^k\right)
  \end{aligned}
  $$
- **Subgradient**ï¼šé‡‡ç”¨ $g_k = X^\top(X\beta_k - y)/n + \lambda s_k$ ä¸ $a_k = a_0/\sqrt{k}$ï¼Œä½“ç° $O(1/\sqrt{k})$ çš„ç†è®ºé€Ÿç‡ (Plain diminishing-step subgradient)ã€‚
- **Continuation Subgradient**ï¼šå°† $\lambda$ ä» $\lambda_{\max}$ é€æ®µé€’å‡ï¼Œæ¯æ®µæ‰§è¡Œå°‘é‡æ¬¡æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿç²—åˆ°ç»†çš„ç»­æ¥ (Multi-stage decreasing-$\lambda$ warm starts)ã€‚
- **Stochastic Subgradient**ï¼šç”¨ mini-batch æ¢¯åº¦ $X_b^\top(X_b\beta - y_b)/|b|$ ä¼°è®¡ $g_k$ï¼Œåœ¨ $1/\sqrt{k}$ æ­¥é•¿ä¸‹å±•ç¤ºå™ªå£°é©±åŠ¨çš„æŒ¯è¡ (Mini-batch stochastic variant)ã€‚
- **Stochastic Proximal Gradient**ï¼šå¯¹ mini-batch æ¢¯åº¦ç«‹å³æ‰§è¡Œè½¯é˜ˆå€¼ï¼Œå…¼å…·éšæœºæ€§ä¸è¿‘ç«¯æ”¶ç¼© (Stochastic proximal shrinkage)ã€‚
- **Primal-Dual Hybrid Gradient (PDHG)**ï¼šæŒ‰ç…§ä»¥ä¸‹å…¬å¼åŒæ­¥æ¨è¿›åŸå§‹ä¸å¯¹å¶ï¼Œå±•ç° Chambolle-Pock å¼è€¦åˆ (Simultaneous primal-dual coupling)ï¼š
  $$
  \begin{aligned}
  d^{k+1} &= \frac{d^k + \sigma(X \bar{\beta}^k - y)}{1 + \sigma n} \\
  \beta^{k+1} &= S_{\tau\lambda}\left(\beta^k - \tau X^{\top} d^{k+1}\right) \\
  \bar{\beta}^{k+1} &= \beta^{k+1} + \theta(\beta^{k+1} - \beta^k)
  \end{aligned}
  $$

## ğŸ¯ å›¾ä¾‹ä¸ç›´è§‰ (Visualization Legend)

| é¢œè‰² | çº¿å‹ | ç®—æ³• | ç›´è§‚è§£è¯» |
| :--- | :--- | :--- | :--- |
| `firebrick` | å®çº¿ | Coordinate Desc | æŒ‡æ•°çº§ä¸‹è½ï¼Œå°‘é‡è¿­ä»£å³è´´è¿‘æœ€ä¼˜ã€‚ |
| `darkred` | è™šçº¿ | Coordinate Desc (Pathwise+Active) | å€ŸåŠ©è·¯å¾„è¿½è¸ªä¸ Active-setï¼Œå¸¸è§â€œä¸€æ­¥åˆ°ä½â€ç°è±¡ã€‚ |
| `limegreen` | å®çº¿ | Huber Gradient | çº¿æ€§æ¸è¿‘ï¼Œå—å¹³æ»‘è¯¯å·®é™åˆ¶åœ¨ 1e-2 å·¦å³ã€‚ |
| `forestgreen` | å®çº¿ | Huber Gradient (Accel) | å‰æœŸæ›´å¿«ï¼ŒåæœŸä»åœç•™åœ¨å¹³æ»‘è¯¯å·®å¹³å°ã€‚ |
| `darkgreen` | è™šçº¿ | Huber (Accel+Restart) | é‡å¯æ¶ˆé™¤å±€éƒ¨éœ‡è¡ï¼Œä½†æé™è¯¯å·®ä»å— Huber è¿‘ä¼¼åˆ¶çº¦ã€‚ |
| `darkblue` | å®çº¿ | FISTA | åŠ¨é‡é©±åŠ¨çš„å¿«é€Ÿä¸‹é™ï¼Œå‡ºç°æ˜æ˜¾â€œé”¯é½¿â€éœ‡è¡ã€‚ |
| `blue` | è™šçº¿ | FISTA (Restart) | è‡ªåŠ¨é‡å¯å‹åˆ¶éœ‡è¡ï¼Œå®ç°å¹³æ»‘é€¼è¿‘ã€‚ |
| `orange` | å®çº¿ | ADMM (Ï=0.5) | æ­¥é•¿åå¤§å¯¼è‡´åæœŸæ‹–å°¾ã€‚ |
| `magenta` | å®çº¿ | ADMM (Ï=1) | ä½ç»´è¡¨ç°æœ€ä½³ï¼Œé€Ÿåº¦ä¸ç¨³å®šæ€§å…¼é¡¾ã€‚ |
| `cyan` | å®çº¿ | ADMM (Ï=2) | é«˜ç»´ä¸‹æ–°çš„æœ€ä½³ç‚¹ï¼ŒæŠ‘åˆ¶éœ‡è¡ã€‚ |
| `purple` | å®çº¿ | ADMM (Ï=5) | æƒ©ç½šè¿‡å¤§ï¼Œæ›´æ–°å¹…åº¦å—é™ã€‚ |
| `brown` | å®çº¿ | Proximal Gradient (ISTA) | å¹³æ»‘ä½†ç•¥æ…¢çš„åŸºçº¿è¿‘ç«¯æ¢¯åº¦ã€‚ |
| `gray` | ç‚¹çº¿ | Subgradient | ç»´æŒåœ¨è¯¯å·®å¹³å°ï¼Œç†è®ºåŸºå‡†ã€‚ |
| `teal` | å®çº¿ | Continuation Subgradient | éš $\lambda$ ç»­æ¥ä½†ä»åœç•™åœ¨å°é˜¶ã€‚ |
| `darkcyan` | ç‚¹çº¿ | Stochastic Subgradient | éšæœºå™ªå£°æ”¾å¤§éœ‡è¡ï¼Œé•¿æ—¶é—´ä¸é™ã€‚ |
| `olive` | è™šçº¿ | Stochastic Proximal Gradient | éšæœºè¿‘ç«¯æ›´æ–°ï¼Œä¸‹é™å¹³ç¼“ã€‚ |
| `black` | å®çº¿ | Primal-Dual Hybrid Gradient | åŸå§‹å¯¹å¶è€¦åˆï¼Œå—æ­¥é•¿é™åˆ¶åœ¨é«˜è¯¯å·®åŒºã€‚ |

---

## ğŸŒ„ åœºæ™¯ Aï¼šä½ç»´æ”¶æ•›å‰–æ (n=200, p=50)

![Low Dim Convergence](ä½ç»´.png)

### ğŸ” å…³é”®æ´å¯Ÿ

* **Pathwise+Active-set**ï¼šå¹³å‡ä¸ä¸­ä½è¿­ä»£æ•°å‡ä¸º 1ï¼ˆ100% è¯•éªŒä¸€æ¬¡åˆ°ä½ï¼‰ï¼Œwarm start + KKT æ£€æŸ¥å®Œå…¨é‡Šæ”¾ç¨€ç–ç»“æ„ã€‚
* **æ ‡å‡†åæ ‡ä¸‹é™**ï¼šå¹³å‡ 4.21 æ¬¡ã€æœ€å 6 æ¬¡ï¼Œå®ç°å¯é¢„æµ‹çš„çº¿æ€§æ”¶æ•›ã€‚
* **FISTA vs. FISTA-R**ï¼š20.33 vs. 12.23 æ¬¡è¿­ä»£ï¼Œé‡å¯ä»¥è½»å¾®åˆæœŸå‡é€Ÿæ¢å–åæœŸç¨³å®šæ€§ã€‚
* **ISTA**ï¼š19.81 æ¬¡ï¼Œä½äº FISTA ä¸ ADMM ä¹‹é—´ï¼ŒéªŒè¯äº†â€œæ— åŠ¨é‡â€çš„å¹³æ»‘ä¸‹é™è½¨è¿¹ã€‚
* **ADMM**ï¼š$\rho=1$ (32.89 æ¬¡) æœ€ä¼˜ï¼Œ$\rho=0.5$ ä¸ $\rho=5$ åˆ†åˆ«è¡¨ç°ä¸ºæ­¥é•¿è¿‡å¤§/è¿‡å°çš„å…¸å‹æ‹–å°¾ã€‚
* **Huber ä¸å„ç±»æ¬¡æ¢¯åº¦/éšæœºç®—æ³•**ï¼šåœ¨ 100 æ­¥é™åˆ¶å†…éƒ½åœç•™åœ¨è¯¯å·®å¹³å°ï¼Œå¼ºè°ƒäº†éå¹³æ»‘é—®é¢˜å¯¹æ­¥é•¿é€‰æ‹©çš„æ•æ„Ÿåº¦ã€‚

### ğŸ“Š æ”¶æ•›ç»Ÿè®¡ ($\mathrm{tol}=10^{-6}$, 100 Trials)

| ç®—æ³• | å¹³å‡è¿­ä»£ | ä¸­ä½ | æœ€å°‘ | æœ€å¤š | æå‰æ”¶æ•› |
| :--- | ---: | ---: | ---: | ---: | ---: |
| Coordinate Desc | 4.21 | 4.00 | 3 | 6 | 100% |
| Coordinate Desc (Pathwise+Active) | **1.00** | **1.00** | 1 | 1 | **100%** |
| Huber Gradient | 100.00 | 100.00 | 100 | 100 | 0% |
| Huber Gradient (Accel) | 100.00 | 100.00 | 100 | 100 | 0% |
| Huber (Accel + Restart) | 100.00 | 100.00 | 100 | 100 | 0% |
| FISTA | 20.33 | 20.00 | 15 | 24 | 100% |
| FISTA (Restart) | 12.23 | 12.00 | 9 | 15 | 100% |
| ADMM (Ï=0.5) | 52.18 | 52.00 | 46 | 61 | 100% |
| ADMM (Ï=1) | 32.89 | 33.00 | 29 | 36 | 100% |
| ADMM (Ï=2) | 43.75 | 43.00 | 36 | 53 | 100% |
| ADMM (Ï=5) | 86.70 | 86.00 | 69 | 100 | 93.0% |
| Proximal Gradient (ISTA) | 19.81 | 19.50 | 13 | 26 | 100% |
| Subgradient | 100.00 | 100.00 | 100 | 100 | 0% |
| Continuation Subgradient | 100.00 | 100.00 | 100 | 100 | 0% |
| Stochastic Subgradient | 100.00 | 100.00 | 100 | 100 | 0% |
| Stochastic Proximal Gradient | 100.00 | 100.00 | 100 | 100 | 0% |
| Primal-Dual Hybrid Gradient | 100.00 | 100.00 | 100 | 100 | 0% |

---

## ğŸŒ‹ åœºæ™¯ Bï¼šé«˜ç»´æ”¶æ•›å‰–æ (n=200, p=1000)

![High Dim Convergence](é«˜ç»´.png)

### ğŸ’¡ å…³é”®è§‚å¯Ÿ

* **ç»´åº¦æå‡åçš„åŠ¨é‡éœ‡è¡**ï¼šFISTA å¹³å‡ 90.40 æ¬¡ã€å‡ºç°å¤§å¹…â€œé”¯é½¿â€ï¼›FISTA-Restart å°†è¿­ä»£å‹ç¼©åˆ° 39.40 æ¬¡å¹¶ä¿æŒå¹³æ»‘ã€‚
* **ADMM æƒ©ç½šç³»æ•°æ¼‚ç§»**ï¼š$\rho=2$ å¹³å‡ 80 æ¬¡æˆä¸ºæ–°çš„ç”œèœœç‚¹ï¼›$\rho=0.5$ åœ¨ 250 æ­¥å†…æ— æ³•è¾¾æ ‡ï¼Œ$\rho=5$ è™½æ”¶æ•›ä½†é€Ÿåº¦æ˜æ˜¾ä¸‹é™ã€‚
* **åŸå§‹-å¯¹å¶ä¸å„ç±»æ¬¡æ¢¯åº¦**ï¼šå—é™äºå›ºå®šæ­¥é•¿/ç»­æ¥ç­–ç•¥ï¼Œå…¨éƒ¨è¾¾åˆ° 250 æ­¥ä¸Šé™ä»æœªæ»¡è¶³ $\mathrm{tol}$ï¼Œå‡¸æ˜¾é«˜ç»´ç—…æ€ä¸‹å¯¹æ›´å¼º preconditioning çš„éœ€æ±‚ã€‚
* **åæ ‡ç±»æ–¹æ³•**ï¼šå³ä½¿åœ¨ $p=1000$ï¼Œæ ‡å‡†åæ ‡ä¸‹é™ä¿æŒ 4 æ¬¡é‡çº§ï¼ŒPathwise+Active ä¾æ—§â€œä¸€æ­¥åˆ°é¡¶â€ï¼Œè¯´æ˜å¯åˆ†ç»“æ„ä¸ warm start å¯¹ç‰¹å¾è§„æ¨¡å‡ ä¹ä¸æ•æ„Ÿã€‚

### ğŸ“Š æ”¶æ•›ç»Ÿè®¡ ($\mathrm{tol}=10^{-6}$, 10 Trials)

| ç®—æ³• | å¹³å‡è¿­ä»£ | ä¸­ä½ | æœ€å°‘ | æœ€å¤š | æå‰æ”¶æ•› |
| :--- | ---: | ---: | ---: | ---: | ---: |
| Coordinate Desc | 4.00 | 4.00 | 3 | 5 | 100% |
| Coordinate Desc (Pathwise+Active) | **1.00** | **1.00** | 1 | 1 | **100%** |
| Huber Gradient | 250.00 | 250.00 | 250 | 250 | 0% |
| Huber Gradient (Accel) | 250.00 | 250.00 | 250 | 250 | 0% |
| Huber (Accel + Restart) | 250.00 | 250.00 | 250 | 250 | 0% |
| FISTA | 90.40 | 91.00 | 81 | 102 | 100% |
| FISTA (Restart) | 39.40 | 39.00 | 37 | 43 | 100% |
| ADMM (Ï=0.5) | 250.00 | 250.00 | 250 | 250 | 0% |
| ADMM (Ï=1) | 138.30 | 138.50 | 132 | 146 | 100% |
| ADMM (Ï=2) | 80.00 | 79.50 | 77 | 84 | 100% |
| ADMM (Ï=5) | 106.00 | 105.50 | 92 | 116 | 100% |
| Proximal Gradient (ISTA) | 120.90 | 118.50 | 109 | 135 | 100% |
| Subgradient | 250.00 | 250.00 | 250 | 250 | 0% |
| Continuation Subgradient | 250.00 | 250.00 | 250 | 250 | 0% |
| Stochastic Subgradient | 250.00 | 250.00 | 250 | 250 | 0% |
| Stochastic Proximal Gradient | 250.00 | 250.00 | 250 | 250 | 0% |
| Primal-Dual Hybrid Gradient | 250.00 | 250.00 | 250 | 250 | 0% |

---

## â±ï¸ æ—¶é—´å¤æ‚åº¦é€è§† (Time Complexity)

| ç®—æ³• | å•æ¬¡è¿­ä»£å¤æ‚åº¦ | ä¸»è¦ç“¶é¢ˆ | å¯¹é«˜ç»´ ($p \gg n$) çš„æ•æ„Ÿåº¦ |
| :--- | :--- | :--- | :--- |
| Coordinate Desc / Pathwise+Active | $O(np)$ | å‘é‡å†…ç§¯ã€æ®‹å·®æ›´æ–° | **çº¿æ€§**ï¼šwarm start ä»¤è¿­ä»£è½®æ•°å‡ ä¹å¸¸æ•°åŒ–ã€‚ |
| FISTA / ISTA / Stochastic Proximal | $O(np)$ æˆ– $O(nb)$ | $X^\top(X\beta - y)$ï¼ˆæˆ– mini-batch ç‰ˆæœ¬ï¼‰ | çº¿æ€§å¢é•¿ï¼ŒåŠ¨é‡/é‡å¯å†³å®šéœ‡è¡æ°´å¹³ã€‚ |
| Huber Gradient å®¶æ— | $O(np)$ | å¹³æ»‘æ¢¯åº¦è®¡ç®— + æ­¥é•¿é™åˆ¶ | çº¿æ€§ä½†å­˜åœ¨è¯¯å·®åœ°æ¿ã€‚ |
| ADMM | é¢„å¤„ç† $O(p^3)$ï¼Œè¿­ä»£ $O(p^2)$ | æ±‚è§£ $(X^\top X + \rho I)^{-1}$ | **ç«‹æ–¹**ï¼šé«˜ç»´ä¸‹æˆä¸ºä¸»è¦ç“¶é¢ˆã€‚ |
| (Stochastic) Subgradient | $O(np)$ æˆ– $O(nb)$ | æ¬¡æ¢¯åº¦ + æ­¥é•¿è°ƒåº¦ | çº¿æ€§ï¼Œä½† $1/\sqrt{k}$ è¡°å‡è¿‡æ…¢ã€‚ |
| Primal-Dual Hybrid Gradient | $O(np)$ | åŸå§‹-å¯¹å¶ä¹˜ç§¯ä¸æŠ•å½± | çº¿æ€§ï¼Œå¯¹æ­¥é•¿è¶…å‚æ•°è¾ƒæ•æ„Ÿã€‚ |

ç»“è®ºï¼šå½“ $p$ ä¸Šå‡åˆ°åƒç»´é‡çº§æ—¶ï¼Œå”¯ä¸€çœŸæ­£çˆ†ç‚¸çš„æ˜¯ ADMM çš„é¢„å¤„ç†ï¼›åæ ‡ç³»ç±»ä¸ï¼ˆéšæœºï¼‰è¿‘ç«¯æ¢¯åº¦åˆ™å‡­å€Ÿå‘é‡åŒ–è¿ç®—ä¿æŒäº†è‰¯å¥½æ‰©å±•æ€§ã€‚

---

## âœ… ç»¼åˆç»“è®º (Conclusion)

1. **è·¯å¾„ + Active-set** æ„æˆå…¨å±€æœ€ç¨³å¥çš„æ”¶æ•›ç­–ç•¥â€”â€”åœ¨ä¸¤ä¸ªåœºæ™¯é‡Œéƒ½ä»¥ 1 æ¬¡è¿­ä»£è§¦è¾¾ $\mathrm{tol}$ã€‚
2. **åŠ¨é‡é‡å¯æ˜¯åŠ¨é‡ç®—æ³•çš„â€œå®‰å…¨å¸¦â€**ï¼šFISTA-R åœ¨é«˜ç»´åœºæ™¯ä¸‹å‡å°‘äº† 2Ã—+ çš„è¿­ä»£å¹¶å½»åº•æ¶ˆé™¤éœ‡è¡ã€‚
3. **ADMM æƒ©ç½šå‚æ•°ä¼šéšç»´åº¦è¿ç§»**ï¼šä½ç»´æ¨è $\rho \approx 1$ï¼Œé«˜ç»´åˆ™éœ€è¦æå‡åˆ° $\rho \approx 2$ æ‰èƒ½å…¼é¡¾é€Ÿåº¦ä¸ç¨³å®šã€‚
4. **è¿‘ç«¯ä¸æ¬¡æ¢¯åº¦çš„è½å·®**ï¼šISTA ä»èƒ½ç¨³å®šä¸‹é™ï¼Œè€Œçº¯æ¬¡æ¢¯åº¦ï¼ˆå«éšæœº/ç»­æ¥ï¼‰ä¸åŸºäºå¹³æ»‘è¿‘ä¼¼çš„ Huber ç³»åˆ—åœ¨å›ºå®šæ­¥é•¿ä¸è¿­ä»£ä¸Šé™ä¸‹éš¾ä»¥ç©¿é€è¯¯å·®å¹³å°ã€‚
5. **åŸå§‹-å¯¹å¶æ··åˆé€‚åˆä½œä¸ºç»“æ„æé†’**ï¼šPDHG çš„è½¨è¿¹è¡¨æ˜ï¼Œè‹¥æ— é¢å¤– preconditioningï¼Œå…¶æ­¥é•¿é€‰æ‹©ä»å—é™ï¼Œä½†åœ¨éœ€è¦æ˜¾å¼å¯¹å¶å˜é‡çš„åœºåˆä¾ç„¶æ˜¯å¯è¡ŒåŸºçº¿ã€‚

---

## ğŸ’» å¤ç°æŒ‡å¼• (How to Reproduce)

1. å®‰è£…ä¾èµ–ï¼š
   ```bash
   pip install numpy matplotlib scikit-learn
   ```
2. è¿è¡Œå®éªŒï¼š
   ```bash
   cd verson_2
   python 2.py
   ```
   è¯¥è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆæ‰€æœ‰éšæœºè¯•éªŒã€è¾“å‡ºæ”¶æ•›ç»Ÿè®¡ (`æ”¶æ•›æƒ…å†µ.md`)ï¼Œå¹¶ç”Ÿæˆ `ä½ç»´.png` ä¸ `é«˜ç»´.png` ä¸¤å¹…æ”¶æ•›æ›²çº¿ã€‚
3. è‹¥éœ€è°ƒæ•´åœºæ™¯æˆ–å‚æ•°ï¼Œå¯ç›´æ¥ä¿®æ”¹ `2.py` é¡¶éƒ¨çš„ç»Ÿä¸€é…ç½®ï¼ˆæ ·æœ¬/ç‰¹å¾è§„æ¨¡ã€è¿­ä»£ä¸Šé™ã€è¯•éªŒæ¬¡æ•°ã€æƒ©ç½šæƒé‡ç­‰ï¼‰ã€‚

ä¿æŒä¸Šè¿°æµç¨‹å³å¯åœ¨æœ¬ç›®å½•è·å¾—å®Œæ•´çš„æ•°å€¼å¯¹æ¯”ç»“æœï¼Œæ–¹ä¾¿åœ¨æŠ¥å‘Šæˆ–æ¼”ç¤ºä¸­å¼•ç”¨ã€‚ç¥ç ”ç©¶é¡ºåˆ©ï¼
